{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import tensorflow as tf\n",
    "\n",
    "import glob\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from keras.models import load_model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=28\n",
    "#training image path\n",
    "DATADIR = \"D://PyProject//Training\"    \n",
    "CATEGORIES = ['char_a', 'char_b','char_c','char_d','char_e','char_f']\n",
    "training_data = []\n",
    "Final_arr = []\n",
    "#feat_data1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data():#this inlcudes preprocessing steps : Grayscaling and Binarization\n",
    "    for category in CATEGORIES:\n",
    "\n",
    "        path = os.path.join(DATADIR, category)  # path to a and b\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            #i=i+1\n",
    "            try:\n",
    "\n",
    "                #GrayScaling\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                reh1, th1 = cv2.threshold(img_array, 0, 255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "                Final_arr = cv2.resize(th1, (IMG_SIZE, IMG_SIZE))\n",
    "                training_data.append([Final_arr, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (556, 2)\n",
      "Total:  556\n"
     ]
    }
   ],
   "source": [
    "print('Shape: ',np.shape(training_data))\n",
    "print('Total: ',len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556\n"
     ]
    }
   ],
   "source": [
    "#training_data=feat_data1\n",
    "no_of_data = len(training_data)\n",
    "print(no_of_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(556, 28, 28)\n",
      "(556,)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for  features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))\n",
    "#print(X)\n",
    "#X = np.array([X]).reshape(no_of_data,IMG_SIZE*IMG_SIZE)  # last one is because img is grayscaled(#27, 576)\n",
    "#y = np.array([y]).reshape(no_of_data,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(444, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "print(np.shape(X_train))\n",
    "y_train[0]\n",
    "X_train = np.array(X_train) \n",
    "X_test = np.array(X_test) \n",
    "y_train = np.array(y_train) \n",
    "y_test = np.array(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b7757c2d68>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.imshow(X_train[0])\n",
    "plt.show()\n",
    "plt.imshow(X_train[0], cmap = plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b7758236d8>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAALrUlEQVR4nO3dX4hc9RnG8eeJcZJgRJLahhBDtZKbUGmsS2iIFoNUYm6iN2IuQgrCeqGg4EXFXuhlKFXpRRFiDabBKoKKuQipaQhERcRdSfPHtI2ViBtiUknEBITJxrcXeyJrsjOzmXNmzrjv9wPLzJ4zs/My5rtn/q0/R4QAzHyz6h4AQH8QO5AEsQNJEDuQBLEDSczu5401Go2YN29eP28yhfHx8Zb7bLe9bqf9nd6tuXDhQtv958+f7/q68+fPb7t/1iyOVZf65ptv1Gw2p/yPWip222sl/UnSVZL+EhGb211+3rx5Wr16dZmbxBROnjzZcl+j0Wh73dmz2/8TaPeLRJLOnTvXdv/Y2FjLfV999VXb6w4NDbXdP2fOnLb7M/4yeO+991ru6/resH2VpD9LukfSckkbbC/v9ucB6K0yv/pWSvokIj6NiKakVyWtr2YsAFUrE/sSSZ9P+n6s2PY9todtj9geaTabJW4OQBk9f1ITEVsiYigihjo9fwTQO2ViPy5p6aTvbyi2ARhAZWL/UNIy2zfZbkh6QNKOasYCULWu33qLiHHbj0j6uybeetsaEYcrmwxApUq9zx4ROyXtrGgWAD2U71MHQFLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJFFqyWbbxySdlXRB0nhEDFUxFIDqlYq9sCYivqzg5wDoIR7GA0mUjT0kvW171PbwVBewPWx7xPZIs9kseXMAulX2YfztEXHc9k8k7bb9r4jYN/kCEbFF0hZJuu6666Lk7QHoUqkje0QcL05PSXpT0soqhgJQva5jt32N7Wsvnpd0t6RDVQ0GoFplHsYvkvSm7Ys/528RsauSqQBUruvYI+JTSb+ocBYAPcRbb0ASxA4kQexAEsQOJEHsQBJV/CEM0NKZM2e6vu6aNWsqnAQc2YEkiB1IgtiBJIgdSILYgSSIHUiC2IEkiB1IgtiBJIgdSILYgSSIHUiC2IEkiB1IgtiBJIgdSILYgSSIHUiC2IEkiB1IgtiBJIgdSILYgSSIHUiiY+y2t9o+ZfvQpG0Lbe+2fbQ4XdDbMQGUNZ0j+0uS1l6y7QlJeyJimaQ9xfcABljH2CNin6TTl2xeL2lbcX6bpHurHQtA1bpd621RRJwozn8haVGrC9oeljQsSXPnzu3y5gCUVfoFuogISdFm/5aIGIqIoUajUfbmAHSp29hP2l4sScXpqepGAtAL3ca+Q9Km4vwmSW9VMw6AXun4nN32K5LulHS97TFJT0naLOk12w9K+kzS/b0cEr3zzjvvlLr+LbfcUtEk6LWOsUfEhha77qp4FgA9xCfogCSIHUiC2IEkiB1IgtiBJLr9uCx+IN5///2e/vyDBw/29OejOhzZgSSIHUiC2IEkiB1IgtiBJIgdSILYgSR4n30GGB0d7fq6d9xxR9v9Zf8EFoODIzuQBLEDSRA7kASxA0kQO5AEsQNJEDuQBO+zo61Vq1a13d/rv5dHdTiyA0kQO5AEsQNJEDuQBLEDSRA7kASxA0nwPvsM1+l98tmzy/0T2LhxY9v927dvL/XzUZ2OR3bbW22fsn1o0ranbR+3vb/4WtfbMQGUNZ2H8S9JWjvF9uciYkXxtbPasQBUrWPsEbFP0uk+zAKgh8q8QPeI7QPFw/wFrS5ke9j2iO2RZrNZ4uYAlNFt7M9LulnSCkknJD3T6oIRsSUihiJiqNFodHlzAMrqKvaIOBkRFyLiW0kvSFpZ7VgAqtZV7LYXT/r2PkmHWl0WwGDo+Car7Vck3Snpettjkp6SdKftFZJC0jFJD/VuRAwy3kf/4egYe0RsmGLziz2YBUAP8XFZIAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAn+V9IzHEsq4yKO7EASxA4kQexAEsQOJEHsQBLEDiRB7EASvM8+A9x2220t942OjvZxksstWNByZTCdOXOm7XX37t3bdv/atVOtN4pWOLIDSRA7kASxA0kQO5AEsQNJEDuQBLEDSfA++wy3atWqtvtnz27/T2B8fLzt/nPnzl3xTFXZtWtX2/3r1q3r0yQ/DB2P7LaX2t5r+2Pbh20/WmxfaHu37aPFaetPTwCo3XQexo9Lejwilkv6laSHbS+X9ISkPRGxTNKe4nsAA6pj7BFxIiI+Ks6flXRE0hJJ6yVtKy62TdK9PZoRQAWu6Dm77Rsl3SrpA0mLIuJEsesLSYtaXGdY0rAkzZ07t+tBAZQz7Vfjbc+X9LqkxyLi68n7IiIkxVTXi4gtETEUEUONRqPUsAC6N63YbV+tidBfjog3is0nbS8u9i+WdKo3IwKoQseH8bYt6UVJRyLi2Um7dkjaJGlzcfpWTybEjDXxT6u1iQeMrXX6E9dZs/gYyWTTec6+WtJGSQdt7y+2PamJyF+z/aCkzyTd35MJAVSiY+wR8a6kVr+C76p2HAC9wuMcIAliB5IgdiAJYgeSIHYgCf7EFQNrzZo1bffPmTOnT5PMDBzZgSSIHUiC2IEkiB1IgtiBJIgdSILYgSSIHUiC2IEkiB1IgtiBJIgdSILYgSSIHUiC2IEkiB1IgtiBJIgdSILYgSSIHUiC2IEkiB1IgtiBJDrGbnup7b22P7Z92PajxfanbR+3vb/4Wtf7cQF0azqLRIxLejwiPrJ9raRR27uLfc9FxB97Nx6AqkxnffYTkk4U58/aPiJpSa8HA1CtK3rObvtGSbdK+qDY9IjtA7a32l7Q4jrDtkdsjzSbzXLTAujatGO3PV/S65Iei4ivJT0v6WZJKzRx5H9mqutFxJaIGIqIoUajUX5iAF2ZVuy2r9ZE6C9HxBuSFBEnI+JCRHwr6QVJK3s3JoCypvNqvCW9KOlIRDw7afviSRe7T9Kh6scDUJXpvBq/WtJGSQdt7y+2PSlpg+0VkkLSMUkP9WA+ABWZzqvx70ryFLt2Vj8OgF7hE3RAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJOGI6N+N2f+T9NmkTddL+rJvA1yZQZ1tUOeSmK1bVc7204j48VQ7+hr7ZTduj0TEUG0DtDGosw3qXBKzdatfs/EwHkiC2IEk6o59S823386gzjaoc0nM1q2+zFbrc3YA/VP3kR1AnxA7kEQtsdtea/vftj+x/UQdM7Ri+5jtg8Uy1CM1z7LV9inbhyZtW2h7t+2jxemUa+zVNNtALOPdZpnxWu+7upc/7/tzdttXSfqPpN9IGpP0oaQNEfFxXwdpwfYxSUMRUfsHMGz/WtI5SX+NiJ8X2/4g6XREbC5+US6IiN8NyGxPSzpX9zLexWpFiycvMy7pXkm/VY33XZu57lcf7rc6juwrJX0SEZ9GRFPSq5LW1zDHwIuIfZJOX7J5vaRtxfltmvjH0nctZhsIEXEiIj4qzp+VdHGZ8VrvuzZz9UUdsS+R9Pmk78c0WOu9h6S3bY/aHq57mCksiogTxfkvJC2qc5gpdFzGu58uWWZ8YO67bpY/L4sX6C53e0T8UtI9kh4uHq4OpJh4DjZI751OaxnvfplimfHv1Hnfdbv8eVl1xH5c0tJJ399QbBsIEXG8OD0l6U0N3lLUJy+uoFucnqp5nu8M0jLeUy0zrgG47+pc/ryO2D+UtMz2TbYbkh6QtKOGOS5j+5rihRPZvkbS3Rq8pah3SNpUnN8k6a0aZ/meQVnGu9Uy46r5vqt9+fOI6PuXpHWaeEX+v5J+X8cMLeb6maR/Fl+H655N0iuaeFh3XhOvbTwo6UeS9kg6KukfkhYO0GzbJR2UdEATYS2uabbbNfEQ/YCk/cXXurrvuzZz9eV+4+OyQBK8QAckQexAEsQOJEHsQBLEDiRB7EASxA4k8X9LopKGY1gUhwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_test = tf.keras.utils.normalize(X_test, axis = 1)\n",
    "plt.imshow(X_train[0], cmap = plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples dimension:  (444, 28, 28, 1)\n",
      "Testing samples dimension:  (112, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_trainr = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "X_testr = np.array(X_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "print('Training samples dimension: ', X_trainr.shape)\n",
    "print('Testing samples dimension: ', X_testr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import  Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3,3), input_shape = X_trainr.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), input_shape = X_trainr.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), input_shape = X_trainr.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D( pool_size = (2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(6))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 198       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 80,934\n",
      "Trainable params: 80,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/44\n",
      "10/10 [==============================] - 1s 39ms/step - loss: 1.7568 - accuracy: 0.2548 - val_loss: 1.7441 - val_accuracy: 0.1940\n",
      "Epoch 2/44\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 1.7090 - accuracy: 0.2516 - val_loss: 1.7250 - val_accuracy: 0.1940\n",
      "Epoch 3/44\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 1.6944 - accuracy: 0.2516 - val_loss: 1.7176 - val_accuracy: 0.1940\n",
      "Epoch 4/44\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 1.6902 - accuracy: 0.2516 - val_loss: 1.7048 - val_accuracy: 0.1940\n",
      "Epoch 5/44\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 1.6780 - accuracy: 0.2516 - val_loss: 1.6906 - val_accuracy: 0.1940\n",
      "Epoch 6/44\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 1.6733 - accuracy: 0.2484 - val_loss: 1.6748 - val_accuracy: 0.2612\n",
      "Epoch 7/44\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 1.6508 - accuracy: 0.2613 - val_loss: 1.6525 - val_accuracy: 0.1866\n",
      "Epoch 8/44\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 1.6065 - accuracy: 0.2710 - val_loss: 1.5933 - val_accuracy: 0.2687\n",
      "Epoch 9/44\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 1.5459 - accuracy: 0.3032 - val_loss: 1.5026 - val_accuracy: 0.3881\n",
      "Epoch 10/44\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 1.4712 - accuracy: 0.3484 - val_loss: 1.4511 - val_accuracy: 0.3955\n",
      "Epoch 11/44\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 1.3788 - accuracy: 0.3742 - val_loss: 1.3374 - val_accuracy: 0.4328\n",
      "Epoch 12/44\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 1.2942 - accuracy: 0.4355 - val_loss: 1.2771 - val_accuracy: 0.4701\n",
      "Epoch 13/44\n",
      "10/10 [==============================] - 0s 34ms/step - loss: 1.2289 - accuracy: 0.5097 - val_loss: 1.2017 - val_accuracy: 0.5522\n",
      "Epoch 14/44\n",
      "10/10 [==============================] - 0s 33ms/step - loss: 1.1458 - accuracy: 0.5710 - val_loss: 1.1344 - val_accuracy: 0.5746\n",
      "Epoch 15/44\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 1.0541 - accuracy: 0.5871 - val_loss: 1.1238 - val_accuracy: 0.5522\n",
      "Epoch 16/44\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 1.0506 - accuracy: 0.5839 - val_loss: 1.0547 - val_accuracy: 0.5821\n",
      "Epoch 17/44\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 1.0167 - accuracy: 0.5871 - val_loss: 0.9935 - val_accuracy: 0.6269\n",
      "Epoch 18/44\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 0.9150 - accuracy: 0.6419 - val_loss: 0.9559 - val_accuracy: 0.6269\n",
      "Epoch 19/44\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.7962 - accuracy: 0.7387 - val_loss: 0.8750 - val_accuracy: 0.6269\n",
      "Epoch 20/44\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.7437 - accuracy: 0.7387 - val_loss: 0.7847 - val_accuracy: 0.6791\n",
      "Epoch 21/44\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 0.6184 - accuracy: 0.8065 - val_loss: 0.8042 - val_accuracy: 0.6866\n",
      "Epoch 22/44\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.5351 - accuracy: 0.8355 - val_loss: 0.7322 - val_accuracy: 0.7463\n",
      "Epoch 23/44\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.4584 - accuracy: 0.8677 - val_loss: 0.6370 - val_accuracy: 0.7687\n",
      "Epoch 24/44\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.3933 - accuracy: 0.9032 - val_loss: 0.6136 - val_accuracy: 0.8060\n",
      "Epoch 25/44\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.3537 - accuracy: 0.9032 - val_loss: 0.6069 - val_accuracy: 0.7761\n",
      "Epoch 26/44\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.3173 - accuracy: 0.9194 - val_loss: 0.6627 - val_accuracy: 0.7463\n",
      "Epoch 27/44\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.2621 - accuracy: 0.9387 - val_loss: 0.5764 - val_accuracy: 0.7761\n",
      "Epoch 28/44\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.2262 - accuracy: 0.9419 - val_loss: 0.5284 - val_accuracy: 0.8582\n",
      "Epoch 29/44\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.2008 - accuracy: 0.9548 - val_loss: 0.5375 - val_accuracy: 0.8134\n",
      "Epoch 30/44\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.1914 - accuracy: 0.9581 - val_loss: 0.7212 - val_accuracy: 0.7388\n",
      "Epoch 31/44\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.1870 - accuracy: 0.9548 - val_loss: 0.5830 - val_accuracy: 0.8060\n",
      "Epoch 32/44\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.1671 - accuracy: 0.9581 - val_loss: 0.5830 - val_accuracy: 0.8433\n",
      "Epoch 33/44\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.1552 - accuracy: 0.9613 - val_loss: 0.5981 - val_accuracy: 0.8358\n",
      "Epoch 34/44\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.1462 - accuracy: 0.9548 - val_loss: 0.5512 - val_accuracy: 0.8433\n",
      "Epoch 35/44\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.1601 - accuracy: 0.9516 - val_loss: 0.6162 - val_accuracy: 0.7985\n",
      "Epoch 36/44\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.1047 - accuracy: 0.9710 - val_loss: 0.6119 - val_accuracy: 0.7985\n",
      "Epoch 37/44\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0984 - accuracy: 0.9742 - val_loss: 0.5778 - val_accuracy: 0.8582\n",
      "Epoch 38/44\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0761 - accuracy: 0.9871 - val_loss: 0.6688 - val_accuracy: 0.7761\n",
      "Epoch 39/44\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0733 - accuracy: 0.9871 - val_loss: 0.5901 - val_accuracy: 0.8657\n",
      "Epoch 40/44\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0659 - accuracy: 0.9871 - val_loss: 0.6022 - val_accuracy: 0.8433\n",
      "Epoch 41/44\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.0539 - accuracy: 0.9903 - val_loss: 0.5691 - val_accuracy: 0.8209\n",
      "Epoch 42/44\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.0596 - accuracy: 0.9871 - val_loss: 0.6509 - val_accuracy: 0.8358\n",
      "Epoch 43/44\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.0753 - accuracy: 0.9806 - val_loss: 0.7858 - val_accuracy: 0.7910\n",
      "Epoch 44/44\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.0724 - accuracy: 0.9871 - val_loss: 0.6865 - val_accuracy: 0.8060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b7758b2ac8>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_trainr, y_train, epochs = 44, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"path = 'C:/Users/Anurag/OneDrive/Desktop/another_b.png'\\nimg = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\\nimg1 = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_AREA)\\nreh2, th2 = cv2.threshold(img1, 0, 255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\\nim2 = tf.keras.utils.normalize(th2, axis = 1)\\nplt.imshow(img1)\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''path = 'C:/Users/Anurag/OneDrive/Desktop/another_b.png'\n",
    "img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "img1 = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_AREA)\n",
    "reh2, th2 = cv2.threshold(img1, 0, 255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "im2 = tf.keras.utils.normalize(th2, axis = 1)\n",
    "plt.imshow(img1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'im2 = np.array(im2).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\\npred  = model1.predict(im2)\\nprint(np.argmax(pred))\\np = np.argmax(pred)\\nif(p == [0]):\\n    print(\\'Character a identified.\\')\\nelif(p == [1]):\\n    print(\"Character b identifed.\")\\nelif(p == [2]):\\n    print(\"Character c identifed.\")'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''im2 = np.array(im2).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "pred  = model1.predict(im2)\n",
    "print(np.argmax(pred))\n",
    "p = np.argmax(pred)\n",
    "if(p == [0]):\n",
    "    print('Character a identified.')\n",
    "elif(p == [1]):\n",
    "    print(\"Character b identifed.\")\n",
    "elif(p == [2]):\n",
    "    print(\"Character c identifed.\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predd(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    img1 = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_AREA)\n",
    "    reh2, th2 = cv2.threshold(img1, 0, 255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    im2 = tf.keras.utils.normalize(th2, axis = 1)\n",
    "    im2 = np.array(im2).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "    pred  = model1.predict(im2)\n",
    "    predicted = np.argmax(pred)\n",
    "    \n",
    "    return predicted\n",
    "    #plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(model1,open('my_model.pkl','wb'))\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
