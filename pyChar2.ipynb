{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import glob\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "import random\n",
    "import tkinter as tk\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "#use case, block diagram, sequence, literature survey\n",
    "#paragraph refernce  srs soft req. system synopsis\n",
    "# 3 topics 3 paragraphs #no of modules \n",
    "IMG_SIZE=28\n",
    "#training image path\n",
    "DATADIR = \"D:\\PyProject\\Training\"    \n",
    "CATEGORIES = [\"char_a\", \"char_b\",\"char_c\"]\n",
    "training_data = []\n",
    "Final_arr = []\n",
    "#testing out with seprate feature vector\n",
    "print(len(training_data))\n",
    "feat_data1=[]\n",
    "feat_data2=[]\n",
    "#Final_arr = []\n",
    "print(np.shape(feat_data1))\n",
    "nof =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data():#this inlcudes preprocessing steps : Grayscaling and Binarization\n",
    "    i=0\n",
    "    for category in CATEGORIES:\n",
    "    \n",
    "        path = os.path.join(DATADIR, category)  # path to a and b\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            #i=i+1\n",
    "            try:\n",
    "                \n",
    "                #GrayScaling\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                #Binarization\n",
    "                reh1, th1 = cv2.threshold(img_array, 0, 255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "                \n",
    "                #Canny edge 1st feature\n",
    "                can_img= cv2.Canny(th1, 100, 200)\n",
    "                new_array = cv2.resize(can_img, (IMG_SIZE, IMG_SIZE))\n",
    "                Final_arr.append(new_array)\n",
    "                \n",
    "                #Hog feature\n",
    "                fd, hog_arr = hog(th1, orientations=8, pixels_per_cell=(8, 8),\n",
    "                                    cells_per_block=(2, 2), visualize=True ,multichannel=False)\n",
    "                hog_array = cv2.resize(hog_arr , (IMG_SIZE, IMG_SIZE))\n",
    "                Final_arr.append(hog_array)\n",
    "                \n",
    "                # Laplacian of guassian (LoG)\n",
    "                '''blur = cv2.GaussianBlur(th1 ,(5,5), 0)\n",
    "                laplacian = cv2.Laplacian(blur, cv2.CV_64F)\n",
    "                laplacian1 = laplacian/laplacian.max()\n",
    "                \n",
    "                lap_arr = cv2.resize(laplacian1 , (IMG_SIZE, IMG_SIZE))\n",
    "                Final_arr.append(lap_arr)'''\n",
    "                \n",
    "                Final_arr = np.array([Final_arr]).resize(1,784*2)\n",
    "                feat_data1.append([Final_arr,class_num])\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "create_training_data()\n",
    "noo= len(Final_arr)\n",
    "print(\"Actual\")\n",
    "print(noo//nof)\n",
    "print(\"Actual x no of features\")\n",
    "print(len(Final_arr))\n",
    "print(np.shape(Final_arr))\n",
    "Final_arr = np.array([Final_arr]).reshape(noo//nof, nof*IMG_SIZE*IMG_SIZE)\n",
    "print(np.shape(Final_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_data = len(training_data)\n",
    "print(no_of_data)\n",
    "\n",
    "training_data=feat_data1\n",
    "    \n",
    "no_of_data=len(training_data)\n",
    "print(len (training_data))\n",
    "print(np.shape(training_data))\n",
    "#print(training_data[0])\n",
    "#training_data = np.array([training_data]).reshape(195,2*IMG_SIZE*IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''img2 = cv2.imread(\"D:\\\\Py\\\\Training\\\\char_a\\\\Screenshot_55.png\",0)\n",
    "\n",
    "#dst = cv2.fastNlMeansDenoising(img2, None,  2, 4, 7)\n",
    "#th ,img_arr = cv2.threshold(img2, 130, 255, cv2.THRESH_BINARY)\n",
    "reh, th = cv2.threshold(img2, 0, 255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "  \n",
    "# Plotting of source and destination image\n",
    "#plt.subplot(121), plt.imshow(img2)\n",
    "#plt.subplot(122), plt.imshow(th)\n",
    "fast = cv2.FastFeatureDetector_create() \n",
    "kp = fast.detect(th,None)\n",
    "img3 = cv2.drawKeypoints(th, kp, None)\n",
    "\n",
    "# Print all default params\n",
    "#print( \"Threshold: {}\".format(fast.getThreshold()) )\n",
    "#print( \"nonmaxSuppression:{}\".format(fast.getNonmaxSuppression()) )\n",
    "#print( \"neighborhood: {}\".format(fast.getType()) )\n",
    "#print( \"Total Keypoints with nonmaxSuppression: {}\".format(len(kp)) )\n",
    "\n",
    "cv2.imshow('fast_true',img3)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Disable nonmaxSuppression\n",
    "fast.setNonmaxSuppression(1)\n",
    "kp = fast.detect(th,None)\n",
    "img4 = cv2.drawKeypoints(th, kp , None)\n",
    "\n",
    "cv2.imshow('fast_fALSE',img4)\n",
    "cv2.waitKey(0)\n",
    "print(kp)'''\n",
    "\n",
    "#print( \"Total Keypoints without nonmaxSuppression: {}\".format(len(kp)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = cv2.imread(\"D:\\\\PyProject\\\\Training\\\\char_b\\\\Screenshot_5.png\",0)\n",
    "\n",
    "#dst = cv2.fastNlMeansDenoising(img2, None,  2, 4, 7)\n",
    "th ,img_arr = cv2.threshold(img2, 130, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "# Apply Gaussian Blur\n",
    "blur = cv2.GaussianBlur(th,(5,5), 0)\n",
    "cv2.imshow('a1',blur)\n",
    "cv2.waitKey(0)\n",
    " \n",
    "# Apply Laplacian operator in some higher datatype\n",
    "laplacian = cv2.Laplacian(blur, cv2.CV_64F)\n",
    "laplacian1 = laplacian/laplacian.max()\n",
    " \n",
    "cv2.imshow('a7',laplacian1)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for  features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "#X = np.array([X]).reshape(195,2*IMG_SIZE*IMG_SIZE)  # last one is because img is grayscaled(#100, 576)\n",
    "y = np.array([y]).resize(noo//3,)\n",
    "#print(np.shape(X))\n",
    "print(np.shape(y))\n",
    "\n",
    "#normalization\n",
    "norm = MinMaxScaler().fit(Final_arr)\n",
    "Final_arr_norm = norm.transform(Final_arr)\n",
    "    \n",
    "print(Final_arr_norm)\n",
    "print(np.shape(Final_arr_norm))\n",
    "print(len(Final_arr_norm))\n",
    "#print(X)\n",
    "#print('size: ',X.shape)\n",
    "#print(y)\n",
    "#print('size: ',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test spilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Final_arr_norm, y, test_size=0.25,random_state=0)\n",
    "#print(X_train)\n",
    "print('test')\n",
    "#print(X_test)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "#clf = RandomForestClassifier(max_depth=5, random_state=0, n_estimators=100)\n",
    "#clf = make_pipeline(StandardScaler(), SVC(gamma='scale'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print('xtest:',X_test)\n",
    "#print('x_train:',X_train)\n",
    "yhat = clf.predict(X_test)\n",
    "acc=accuracy_score(y_test, yhat)\n",
    "print('Accuracy: ', acc*100,'%')\n",
    "#print(yhat)\n",
    "#print(yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_test = []\n",
    "#GrayScaling\n",
    "test_img = cv2.imread(\"D:\\Py Project\\Training\\char_c\\Screenshot_1.png\",0)\n",
    "#Binarization\n",
    "th, test_img_arr =cv2.threshold(test_img, 0, 255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "# Canny edge\n",
    "t_img=cv2.Canny(test_img_arr,100,200)\n",
    "t_array = cv2.resize(t_img, (1, 28*28*3))\n",
    "XX_test.append(t_array)\n",
    "\n",
    "\n",
    "# Hog\n",
    "fd, hog_image = hog(test_img_arr, orientations=8, pixels_per_cell=(8, 8),\n",
    "                                    cells_per_block=(2, 2), visualize=True ,multichannel=False)\n",
    "test_array = cv2.resize(hog_image, (1, 28*28*3))\n",
    "XX_test.append(test_array)\n",
    "\n",
    "#Laplacian of Gassian\n",
    "blur1 = cv2.GaussianBlur(test_img_arr, (3,3), cv2.BORDER_DEFAULT)\n",
    "laplacian2 = cv2.Laplacian(blur1, cv2.CV_64F)\n",
    "laplacian3 = laplacian2 / laplacian2.max()\n",
    "lap_arr1 = cv2.resize(laplacian3 , (1, 28*28*3))\n",
    "XX_test.append(lap_arr1)\n",
    "print(np.shape(XX_test))\n",
    "\n",
    "\n",
    "XX_test = np.array([XX_test]).resize(1, 3*IMG_SIZE*IMG_SIZE)\n",
    "\n",
    "#normalization\n",
    "norm1 = MinMaxScaler().fit(XX_test)\n",
    "XX_test_norm = norm1.transform(XX_test)\n",
    "print(XX_test.shape)\n",
    "predicted = clf.predict(XX_test_norm)\n",
    "print(predicted.shape)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(predicted == [0]):\n",
    "    print('Character a identified.')\n",
    "elif(predicted == [1]):\n",
    "    print(\"Character b identifed.\")\n",
    "elif(predicted == [2]):\n",
    "    print(\"Character c identifed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
